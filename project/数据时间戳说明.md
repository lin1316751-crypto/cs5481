# ⏰ 数据时间戳说明

**重要**: 所有抓取的数据都保存了原始发布时间!

---

## ✅ 时间戳字段汇总

| 数据源 | 时间字段 | 格式 | 说明 |
|--------|---------|------|------|
| **Reddit** | `timestamp` | Unix时间戳 | 帖子/评论的发布时间 (`created_utc`) |
| **RSS** | `timestamp`<br>`published`<br>`crawl_timestamp` | Unix时间戳 | 文章发布时间<br>原始发布时间<br>抓取时间 |
| **NewsAPI** | `timestamp`<br>`published_at` | Unix时间戳<br>ISO 8601 | 文章发布时间<br>原始时间字符串 |
| **Twitter** | `timestamp` | Unix时间戳 | 推文发布时间 (`created_at`) |
| **StockTwits** | `timestamp` | Unix时间戳 | 消息发布时间 (`created_at`) |

---

## 📊 实际示例

### Reddit 数据
```json
{
  "text": "Tesla stock analysis...",
  "source": "reddit_post",
  "timestamp": 1729411200,  // ← 发布时间 (2024-10-20 08:00:00)
  "post_id": "xyz123",
  "subreddit": "investing"
}
```

**转换为人类可读时间**:
```python
from datetime import datetime
dt = datetime.fromtimestamp(1729411200)
print(dt)  # 2024-10-20 08:00:00
```

### RSS 数据
```json
{
  "text": "Federal Reserve announces...",
  "source": "rss",
  "timestamp": 1729411200,      // ← 发布时间
  "published": 1729411200,      // ← 原始发布时间
  "crawl_timestamp": 1729414800, // ← 抓取时间 (1小时后)
  "url": "https://..."
}
```

**时间差分析**:
```python
publish_time = datetime.fromtimestamp(1729411200)  # 8:00
crawl_time = datetime.fromtimestamp(1729414800)    # 9:00
delay = crawl_time - publish_time  # 1小时延迟
```

### NewsAPI 数据
```json
{
  "text": "Stock market today...",
  "source": "newsapi",
  "timestamp": 1729411200,              // ← Unix时间戳
  "published_at": "2024-10-20T08:00:00Z", // ← ISO格式
  "title": "Market Update"
}
```

### Twitter 数据
```json
{
  "text": "$SPY breaking out!",
  "source": "twitter",
  "timestamp": 1729411200,  // ← 推文发布时间
  "tweet_id": "1234567890",
  "author": "trader_joe"
}
```

---

## 🔍 时间范围验证

### 检查数据新鲜度

**方法 1: 导出后检查**
```python
import json
from datetime import datetime

# 读取导出的数据
with open('data_exports/data_export_xxx.json') as f:
    data = json.load(f)

# 检查最新和最旧的数据
timestamps = [item['timestamp'] for item in data]
newest = datetime.fromtimestamp(max(timestamps))
oldest = datetime.fromtimestamp(min(timestamps))

print(f"最新数据: {newest}")
print(f"最旧数据: {oldest}")
print(f"时间跨度: {newest - oldest}")
```

**方法 2: Redis 实时检查**
```python
import redis
import json
from datetime import datetime

r = redis.Redis(host='localhost', port=6379, decode_responses=True)
items = r.lrange('data_queue', 0, 100)  # 取最新100条

timestamps = []
for item in items:
    data = json.loads(item)
    timestamps.append(data.get('timestamp', 0))

if timestamps:
    newest = datetime.fromtimestamp(max(timestamps))
    print(f"最新数据时间: {newest}")
```

---

## ⏱️ 当前抓取策略

### Reddit
- **API**: `subreddit.new(limit=50)`
- **时间范围**: 最新 50 条帖子
- **发布时间**: 通常在 **几分钟到几小时** 内
- ✅ **实时性**: 优秀

### RSS
- **抓取**: 所有 Feed 中的文章
- **发布时间**: Feed 中通常包含 **最近 24-48 小时** 的文章
- ✅ **实时性**: 良好

### NewsAPI
- **参数**: `from_param=今天` (已优化)
- **发布时间**: **今天 0 点后** 的文章
- ⚠️ **问题**: 默认抓取 1 天前,需要优化

### Twitter X API
- **端点**: `search/recent`
- **时间范围**: 最近 **7 天**
- **排序**: 按相关性或时间
- ⚠️ **问题**: 可能包含几天前的推文

### StockTwits
- **API**: `streams/symbol/{symbol}`
- **时间范围**: 最新 30 条消息
- **发布时间**: 通常在 **几分钟到几小时** 内
- ✅ **实时性**: 优秀

---

## 🚨 需要优化的地方

### 1. NewsAPI 优化 ⚠️ 重要

**当前问题**: 
```python
days_back = self.config.get('days_back', 1)  # 默认 1 天前
from_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')
```

**建议修改**:
```python
days_back = 0  # 改为今天
# 或者使用小时级精度
from_date = (datetime.now() - timedelta(hours=2)).isoformat()
```

**影响**: 
- ❌ 当前可能抓到昨天的新闻
- ✅ 应该抓取最近 1-2 小时的新闻

### 2. Twitter 排序优化 ⚠️ 中等

**当前问题**: 
Twitter API 默认按相关性排序,不一定是最新

**建议优化**:
```python
# 添加时间过滤
params = {
    "query": f"{query} -is:retweet",  # 排除转推
    "max_results": 10,
    "start_time": (datetime.now() - timedelta(hours=6)).isoformat() + "Z"  # 最近6小时
}
```

---

## 📈 数据新鲜度分析

### 预期时间分布

**正常情况** (每小时运行一次):

| 数据源 | 数据年龄 | 说明 |
|--------|---------|------|
| Reddit | 0-60分钟 | 每小时抓最新50条 |
| RSS | 0-2小时 | Feed更新频率 |
| NewsAPI | **0-24小时** | ⚠️ 需优化为 0-2小时 |
| Twitter | 0-7天 | ⚠️ 需优化为 0-6小时 |
| StockTwits | 0-60分钟 | 最新30条消息 |

**优化后**:

| 数据源 | 数据年龄 | 改进 |
|--------|---------|------|
| Reddit | 0-60分钟 | 不变 |
| RSS | 0-2小时 | 不变 |
| NewsAPI | **0-2小时** | ✅ 优化 |
| Twitter | **0-6小时** | ✅ 优化 |
| StockTwits | 0-60分钟 | 不变 |

---

## 🎯 优化建议

### 立即优化

**1. NewsAPI 改为今天**
```yaml
# config.yaml 添加
newsapi:
  days_back: 0  # 今天,不是昨天
```

**2. Twitter 添加时间过滤**
```python
# twitter_v2_crawler.py
# 添加 start_time 参数
```

### 监控脚本

创建 `check_data_freshness.py`:
```python
import redis
import json
from datetime import datetime, timedelta

r = redis.Redis(host='localhost', port=6379, decode_responses=True)
items = r.lrange('data_queue', 0, 1000)

# 按数据源分组
sources = {}
for item in items:
    data = json.loads(item)
    source = data.get('source', 'unknown')
    timestamp = data.get('timestamp', 0)
    
    if source not in sources:
        sources[source] = []
    sources[source].append(timestamp)

# 分析每个数据源
now = datetime.now()
for source, timestamps in sources.items():
    if not timestamps:
        continue
    
    newest = datetime.fromtimestamp(max(timestamps))
    oldest = datetime.fromtimestamp(min(timestamps))
    avg_age = (now - newest).total_seconds() / 3600
    
    print(f"\n{source}:")
    print(f"  数据量: {len(timestamps)}")
    print(f"  最新: {newest} (距今 {avg_age:.1f} 小时)")
    print(f"  最旧: {oldest}")
    print(f"  跨度: {newest - oldest}")
```

---

## ✅ 总结

**好消息**:
- ✅ 所有数据都保存了发布时间
- ✅ 使用标准 Unix 时间戳 (易于分析)
- ✅ 大部分数据源实时性良好

**需要优化**:
- ⚠️ NewsAPI: 改为抓取今天的数据
- ⚠️ Twitter: 添加时间过滤 (最近6小时)

**数据可用性**:
- ✅ 可以按时间排序
- ✅ 可以过滤特定时间段
- ✅ 可以分析发布延迟
- ✅ 支持时间序列分析

---

**下一步**: 立即优化 NewsAPI 和 Twitter 的时间参数!
