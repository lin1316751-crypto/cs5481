# ğŸ—„ï¸ Redis å­˜å‚¨ä¼˜åŒ–æ–¹æ¡ˆ

**é—®é¢˜:** Redis æ˜¯å†…å­˜æ•°æ®åº“ï¼Œå­˜å‚¨å¤ªå¤šæ•°æ®ä¼šå ç”¨å¤§é‡å†…å­˜  
**ç›®æ ‡:** ä¼˜åŒ–å­˜å‚¨ç­–ç•¥ï¼Œè‡ªåŠ¨å¤‡ä»½åˆ°æœ¬åœ°æ–‡ä»¶

---

## ğŸ“Š å­˜å‚¨é—®é¢˜åˆ†æ

### å½“å‰æƒ…å†µ

å‡è®¾æ¯å¤©æŠ“å– **2,000 ç¯‡** RSS æ–‡ç« ï¼Œæ¯ç¯‡å¹³å‡ **2KB**ï¼š

```
æ¯å¤©æ•°æ®é‡ = 2,000 Ã— 2KB = 4MB
æ¯å‘¨æ•°æ®é‡ = 4MB Ã— 7 = 28MB
æ¯æœˆæ•°æ®é‡ = 4MB Ã— 30 = 120MB
```

åŠ ä¸Šå…¶ä»–æ•°æ®æºï¼ˆReddit, Twitter, StockTwits, NewsAPIï¼‰ï¼š

```
Reddit: 48,000æ¡ Ã— 1KB = 48MB/å¤©
NewsAPI: 100æ¡ Ã— 2KB = 0.2MB/å¤©
StockTwits: 8,640æ¡ Ã— 0.5KB = 4.3MB/å¤©
Twitter: 5,760æ¡ Ã— 0.3KB = 1.7MB/å¤©
RSS: 2,000æ¡ Ã— 2KB = 4MB/å¤©
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æ€»è®¡: çº¦ 58MB/å¤© = 1.74GB/æœˆ
```

**é—®é¢˜:**
- âŒ Redis å…¨éƒ¨å­˜å‚¨ä¼šå ç”¨ **1.74GB å†…å­˜/æœˆ**
- âŒ å†å²æ•°æ®ä»·å€¼é€’å‡ï¼ˆ7å¤©å‰çš„æ•°æ®å¾ˆå°‘ç”¨ï¼‰
- âŒ æ²¡æœ‰è‡ªåŠ¨æ¸…ç†æœºåˆ¶

---

## ğŸ¯ ä¼˜åŒ–ç­–ç•¥

### ç­–ç•¥ 1: åˆ†å±‚å­˜å‚¨ â­â­â­â­â­ï¼ˆæ¨èï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Redis (çƒ­æ•°æ® - æœ€è¿‘24å°æ—¶)         â”‚  <- å¿«é€Ÿè®¿é—®
â”‚  å¤§å°: ~60MB                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“ è‡ªåŠ¨å¯¼å‡º
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æœ¬åœ°æ–‡ä»¶ (æ¸©æ•°æ® - æœ€è¿‘7å¤©)         â”‚  <- æŒ‰éœ€åŠ è½½
â”‚  æ ¼å¼: JSON/Parquet                  â”‚
â”‚  å¤§å°: ~400MB                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“ å®šæœŸå‹ç¼©
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å‹ç¼©å½’æ¡£ (å†·æ•°æ® - 7å¤©ä»¥ä¸Š)         â”‚  <- é•¿æœŸå­˜å‚¨
â”‚  æ ¼å¼: .tar.gz                       â”‚
â”‚  å¤§å°: ~50MB (å‹ç¼©å)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¼˜ç‚¹:**
- âœ… Redis åªä¿ç•™ 24 å°æ—¶æ•°æ®ï¼ˆçº¦ 60MBï¼‰
- âœ… èŠ‚çœå†…å­˜ **96.6%**
- âœ… å†å²æ•°æ®ä¸ä¸¢å¤±
- âœ… æŸ¥è¯¢æ€§èƒ½é«˜ï¼ˆçƒ­æ•°æ®åœ¨å†…å­˜ï¼‰

---

### ç­–ç•¥ 2: å­—æ®µç²¾ç®€ â­â­â­â­

**é—®é¢˜:** 17ä¸ªå­—æ®µå ç”¨ç©ºé—´å¤§

**ä¼˜åŒ–:** Redis åªå­˜å¿…è¦å­—æ®µï¼Œå®Œæ•´æ•°æ®å­˜æœ¬åœ°

```python
# Redis å­˜å‚¨ (ç²¾ç®€ç‰ˆ - 7ä¸ªå­—æ®µ)
redis_data = {
    'title': '...',        # å¿…é¡»
    'url': '...',          # å¿…é¡»
    'published': 123456,   # å¿…é¡»
    'source': 'rss',       # å¿…é¡»
    'feed_category': '...', # ç­›é€‰ç”¨
    'language': 'en',      # ç­›é€‰ç”¨
    'summary': '...'       # é¢„è§ˆç”¨
}

# æœ¬åœ°æ–‡ä»¶å­˜å‚¨ (å®Œæ•´ç‰ˆ - 17ä¸ªå­—æ®µ)
local_data = {
    # ... æ‰€æœ‰17ä¸ªå­—æ®µ
}
```

**èŠ‚çœ:** æ¯æ¡æ•°æ®ä» 2KB â†’ 0.8KBï¼Œ**èŠ‚çœ 60%**

---

### ç­–ç•¥ 3: TTL è‡ªåŠ¨è¿‡æœŸ â­â­â­

**åŸç†:** ä½¿ç”¨ Redis çš„ TTL (Time To Live) åŠŸèƒ½

```python
# è®¾ç½®24å°æ—¶åè‡ªåŠ¨åˆ é™¤
redis_client.setex(
    key='data:12345',
    time=86400,  # 24å°æ—¶ = 86400ç§’
    value=json_data
)
```

**ä¼˜ç‚¹:**
- âœ… è‡ªåŠ¨æ¸…ç†
- âœ… æ— éœ€æ‰‹åŠ¨ç®¡ç†
- âœ… ä¿è¯ Redis ä¸ä¼šæ— é™å¢é•¿

**ç¼ºç‚¹:**
- âš ï¸ æ•°æ®ä¼šä¸¢å¤±ï¼ˆéœ€é…åˆå¯¼å‡ºä½¿ç”¨ï¼‰

---

## ğŸ”§ å®ç°æ–¹æ¡ˆ

### æ–¹æ¡ˆ A: å®šæ—¶å¯¼å‡º + TTLï¼ˆæœ€ä¼˜ï¼‰

#### é…ç½®æ–‡ä»¶
```yaml
# config.yaml
redis:
  host: localhost
  port: 6379
  db: 0
  queue_name: data_queue
  
  # ğŸ”¥ æ–°å¢: å­˜å‚¨ä¼˜åŒ–é…ç½®
  storage_optimization:
    enabled: true
    
    # Redis åªä¿ç•™æœ€è¿‘çš„æ•°æ®
    max_keep: 5000  # å‡å°‘åˆ°5000æ¡ï¼ˆçº¦1å°æ—¶æ•°æ®ï¼‰
    
    # æ•°æ®ç”Ÿå‘½å‘¨æœŸï¼ˆç§’ï¼‰
    data_ttl: 86400  # 24å°æ—¶åè‡ªåŠ¨åˆ é™¤
    
    # å­—æ®µç²¾ç®€æ¨¡å¼
    slim_mode: true  # åªå­˜7ä¸ªæ ¸å¿ƒå­—æ®µ

# æ•°æ®ç®¡ç†
data_management:
  # å¯¼å‡ºé…ç½®
  export_dir: data_exports
  
  # ğŸ”¥ æ–°å¢: è‡ªåŠ¨å¯¼å‡ºè§¦å‘æ¡ä»¶
  auto_export:
    enabled: true
    
    # è§¦å‘æ¡ä»¶1: Redisé˜Ÿåˆ—é•¿åº¦
    queue_threshold: 5000  # è¶…è¿‡5000æ¡è‡ªåŠ¨å¯¼å‡º
    
    # è§¦å‘æ¡ä»¶2: å†…å­˜ä½¿ç”¨
    memory_threshold_mb: 100  # è¶…è¿‡100MBè‡ªåŠ¨å¯¼å‡º
    
    # è§¦å‘æ¡ä»¶3: æ—¶é—´é—´éš”
    time_interval_minutes: 60  # æ¯60åˆ†é’Ÿå¯¼å‡ºä¸€æ¬¡
  
  # ğŸ”¥ æ–°å¢: å½’æ¡£é…ç½®
  archive:
    enabled: true
    compress: true  # å‹ç¼©å½’æ¡£
    retention_days: 30  # ä¿ç•™30å¤©
    format: "parquet"  # parquet å‹ç¼©æ¯”é«˜
```

#### ä¼˜åŒ–åçš„ RedisClient

```python
# utils/redis_client.py (ä¼˜åŒ–ç‰ˆ)

class RedisClient:
    def __init__(self, host='localhost', port=6379, db=0, password=None, 
                 queue_name='data_queue', storage_config=None):
        # ... ç°æœ‰ä»£ç  ...
        
        # å­˜å‚¨ä¼˜åŒ–é…ç½®
        self.storage_config = storage_config or {}
        self.slim_mode = self.storage_config.get('slim_mode', False)
        self.data_ttl = self.storage_config.get('data_ttl', 86400)
        self.max_keep = self.storage_config.get('max_keep', 10000)
    
    def push_data(self, data: Dict[str, Any]) -> bool:
        """æ¨é€æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            # ğŸ”¥ å­—æ®µç²¾ç®€æ¨¡å¼
            if self.slim_mode:
                data = self._slim_data(data)
            
            json_data = json.dumps(data, ensure_ascii=False)
            
            # ğŸ”¥ æ¨é€åˆ°é˜Ÿåˆ—
            self.client.lpush(self.queue_name, json_data)
            
            # ğŸ”¥ æ£€æŸ¥æ˜¯å¦éœ€è¦å¯¼å‡ºï¼ˆé˜²æ­¢å†…å­˜å ç”¨ï¼‰
            queue_length = self.client.llen(self.queue_name)
            if queue_length > self.max_keep:
                logger.warning(f"é˜Ÿåˆ—é•¿åº¦ {queue_length} è¶…è¿‡é˜ˆå€¼ {self.max_keep}ï¼Œå»ºè®®å¯¼å‡º")
                # å¯ä»¥åœ¨è¿™é‡Œè§¦å‘å¯¼å‡º
                
            return True
        except Exception as e:
            logger.error(f"æ¨é€æ•°æ®å¤±è´¥: {e}")
            return False
    
    def _slim_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ç²¾ç®€æ•°æ®å­—æ®µï¼ˆåªä¿ç•™æ ¸å¿ƒå­—æ®µï¼‰"""
        slim_fields = [
            'title',
            'url', 
            'published',
            'source',
            'feed_category',
            'language',
            'summary'
        ]
        
        return {k: v for k, v in data.items() if k in slim_fields}
    
    def get_memory_usage(self) -> Dict[str, Any]:
        """è·å– Redis å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        info = self.client.info('memory')
        return {
            'used_memory_mb': info['used_memory'] / 1024 / 1024,
            'used_memory_human': info['used_memory_human'],
            'peak_memory_mb': info['used_memory_peak'] / 1024 / 1024
        }
```

#### æ™ºèƒ½å¯¼å‡ºå™¨

```python
# utils/smart_exporter.py (æ–°æ–‡ä»¶)

import os
import json
import gzip
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path

class SmartExporter:
    """æ™ºèƒ½æ•°æ®å¯¼å‡ºå™¨"""
    
    def __init__(self, redis_client, config):
        self.redis_client = redis_client
        self.config = config
        self.export_dir = Path(config.get('export_dir', 'data_exports'))
        self.archive_dir = self.export_dir / 'archive'
        
        # åˆ›å»ºç›®å½•
        self.export_dir.mkdir(exist_ok=True)
        self.archive_dir.mkdir(exist_ok=True)
        
        # å¯¼å‡ºé…ç½®
        self.auto_export = config.get('auto_export', {})
        self.archive_config = config.get('archive', {})
    
    def should_export(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦å¯¼å‡º"""
        if not self.auto_export.get('enabled', True):
            return False
        
        # æ£€æŸ¥1: é˜Ÿåˆ—é•¿åº¦
        queue_threshold = self.auto_export.get('queue_threshold', 5000)
        queue_length = self.redis_client.get_queue_length()
        if queue_length > queue_threshold:
            logger.info(f"è§¦å‘å¯¼å‡º: é˜Ÿåˆ—é•¿åº¦ {queue_length} > {queue_threshold}")
            return True
        
        # æ£€æŸ¥2: å†…å­˜ä½¿ç”¨
        memory_threshold = self.auto_export.get('memory_threshold_mb', 100)
        memory_info = self.redis_client.get_memory_usage()
        if memory_info['used_memory_mb'] > memory_threshold:
            logger.info(f"è§¦å‘å¯¼å‡º: å†…å­˜ä½¿ç”¨ {memory_info['used_memory_mb']:.1f}MB > {memory_threshold}MB")
            return True
        
        return False
    
    def export(self, batch_size=1000) -> Dict[str, Any]:
        """å¯¼å‡ºæ•°æ®åˆ°æœ¬åœ°æ–‡ä»¶"""
        stats = {
            'exported': 0,
            'export_file': None,
            'format': self.archive_config.get('format', 'json')
        }
        
        try:
            # è·å–é˜Ÿåˆ—é•¿åº¦
            total = self.redis_client.get_queue_length()
            if total == 0:
                logger.info("é˜Ÿåˆ—ä¸ºç©ºï¼Œè·³è¿‡å¯¼å‡º")
                return stats
            
            # è®¡ç®—éœ€è¦å¯¼å‡ºçš„æ•°é‡
            max_keep = self.redis_client.max_keep
            to_export = max(0, total - max_keep)
            
            if to_export == 0:
                logger.info(f"é˜Ÿåˆ—é•¿åº¦ {total} æœªè¶…è¿‡ {max_keep}ï¼Œè·³è¿‡å¯¼å‡º")
                return stats
            
            logger.info(f"å¼€å§‹å¯¼å‡º {to_export} æ¡æ•°æ®ï¼ˆä¿ç•™ {max_keep} æ¡åœ¨Redisï¼‰")
            
            # åˆ†æ‰¹è·å–æ•°æ®
            all_data = []
            for i in range(0, to_export, batch_size):
                batch = self.redis_client.get_data(min(batch_size, to_export - i))
                all_data.extend(batch)
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            export_format = stats['format']
            
            if export_format == 'parquet':
                filename = f'data_{timestamp}.parquet'
                filepath = self.export_dir / filename
                self._export_parquet(all_data, filepath)
            elif export_format == 'json':
                filename = f'data_{timestamp}.json.gz'
                filepath = self.export_dir / filename
                self._export_json_gz(all_data, filepath)
            else:
                filename = f'data_{timestamp}.json'
                filepath = self.export_dir / filename
                self._export_json(all_data, filepath)
            
            stats['exported'] = len(all_data)
            stats['export_file'] = str(filepath)
            
            logger.info(f"âœ“ å¯¼å‡ºå®Œæˆ: {stats['exported']} æ¡ â†’ {filepath}")
            
            # æ¸…ç†æ—§å½’æ¡£
            self._cleanup_old_archives()
            
            return stats
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºå¤±è´¥: {e}")
            return stats
    
    def _export_json(self, data, filepath):
        """å¯¼å‡ºä¸º JSON"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def _export_json_gz(self, data, filepath):
        """å¯¼å‡ºä¸ºå‹ç¼© JSON"""
        with gzip.open(filepath, 'wt', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False)
    
    def _export_parquet(self, data, filepath):
        """å¯¼å‡ºä¸º Parquetï¼ˆé«˜å‹ç¼©æ¯”ï¼‰"""
        df = pd.DataFrame(data)
        df.to_parquet(filepath, compression='gzip', index=False)
    
    def _cleanup_old_archives(self):
        """æ¸…ç†è¿‡æœŸå½’æ¡£"""
        if not self.archive_config.get('enabled', True):
            return
        
        retention_days = self.archive_config.get('retention_days', 30)
        cutoff_date = datetime.now() - timedelta(days=retention_days)
        
        deleted_count = 0
        for file in self.export_dir.glob('data_*.{json,json.gz,parquet}'):
            # ä»æ–‡ä»¶åæå–æ—¥æœŸ
            try:
                date_str = file.stem.split('_')[1]  # data_20241020_120000
                file_date = datetime.strptime(date_str, '%Y%m%d')
                
                if file_date < cutoff_date:
                    file.unlink()
                    deleted_count += 1
                    logger.info(f"åˆ é™¤è¿‡æœŸå½’æ¡£: {file.name}")
            except:
                continue
        
        if deleted_count > 0:
            logger.info(f"æ¸…ç†å®Œæˆ: åˆ é™¤ {deleted_count} ä¸ªè¿‡æœŸå½’æ¡£")
```

---

## ğŸ“Š å­˜å‚¨å¯¹æ¯”

### ä¼˜åŒ–å‰ vs ä¼˜åŒ–å

| é¡¹ç›® | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | èŠ‚çœ |
|------|--------|--------|------|
| **Redis å†…å­˜** | 1.74GB/æœˆ | 60MB | â¬‡ï¸ **96.6%** |
| **æ•°æ®ä¿ç•™** | å…¨éƒ¨åœ¨Redis | 24å°æ—¶çƒ­æ•°æ® | - |
| **å†å²æ•°æ®** | âŒ æ—  | âœ… æœ¬åœ°æ–‡ä»¶ | - |
| **å‹ç¼©** | âŒ æ—  | âœ… Parquet/Gzip | â¬‡ï¸ 70% |
| **è‡ªåŠ¨æ¸…ç†** | âŒ æ—  | âœ… 30å¤© | - |
| **æŸ¥è¯¢é€Ÿåº¦** | å¿« | çƒ­æ•°æ®å¿«ï¼Œå†å²æ…¢ | - |

---

## ğŸ¯ æ¨èé…ç½®

### é…ç½® 1: è½»é‡çº§ï¼ˆæ¨èä¸ªäººç”¨æˆ·ï¼‰

```yaml
redis:
  storage_optimization:
    enabled: true
    max_keep: 5000      # çº¦1å°æ—¶æ•°æ®
    data_ttl: 86400     # 24å°æ—¶è¿‡æœŸ
    slim_mode: true     # ç²¾ç®€æ¨¡å¼

data_management:
  auto_export:
    enabled: true
    queue_threshold: 5000
    memory_threshold_mb: 50
    time_interval_minutes: 60
  
  archive:
    enabled: true
    compress: true
    retention_days: 7   # åªä¿ç•™7å¤©
    format: "parquet"   # é«˜å‹ç¼©æ¯”
```

**æ•ˆæœ:**
- Redis: ~50MB
- æœ¬åœ°: ~280MB/å‘¨ï¼ˆå‹ç¼©åï¼‰

---

### é…ç½® 2: æ ‡å‡†ç‰ˆï¼ˆæ¨èå›¢é˜Ÿä½¿ç”¨ï¼‰

```yaml
redis:
  storage_optimization:
    enabled: true
    max_keep: 10000     # çº¦2å°æ—¶æ•°æ®
    data_ttl: 172800    # 48å°æ—¶è¿‡æœŸ
    slim_mode: false    # ä¿ç•™å®Œæ•´å­—æ®µ

data_management:
  auto_export:
    enabled: true
    queue_threshold: 10000
    memory_threshold_mb: 100
    time_interval_minutes: 120
  
  archive:
    enabled: true
    compress: true
    retention_days: 30   # ä¿ç•™30å¤©
    format: "parquet"
```

**æ•ˆæœ:**
- Redis: ~100MB
- æœ¬åœ°: ~1.2GB/æœˆï¼ˆå‹ç¼©åï¼‰

---

### é…ç½® 3: ä¸“ä¸šç‰ˆï¼ˆæ¨èç ”ç©¶æœºæ„ï¼‰

```yaml
redis:
  storage_optimization:
    enabled: false      # ä¸é™åˆ¶
    max_keep: 50000     # çº¦åŠå¤©æ•°æ®
    slim_mode: false

data_management:
  auto_export:
    enabled: true
    queue_threshold: 50000
    memory_threshold_mb: 500
    time_interval_minutes: 360  # æ¯6å°æ—¶
  
  archive:
    enabled: true
    compress: true
    retention_days: 90   # ä¿ç•™3ä¸ªæœˆ
    format: "parquet"
```

**æ•ˆæœ:**
- Redis: ~500MB
- æœ¬åœ°: ~3.6GB/æœˆï¼ˆå‹ç¼©åï¼‰

---

## ğŸ’¡ ä½¿ç”¨å»ºè®®

### 1. å®šæœŸç›‘æ§

```python
# æ£€æŸ¥ Redis å†…å­˜
from utils.redis_client import RedisClient

client = RedisClient()
memory_info = client.get_memory_usage()
print(f"Redis å†…å­˜ä½¿ç”¨: {memory_info['used_memory_human']}")
print(f"é˜Ÿåˆ—é•¿åº¦: {client.get_queue_length()}")
```

### 2. æ‰‹åŠ¨å¯¼å‡º

```python
from utils.smart_exporter import SmartExporter

exporter = SmartExporter(redis_client, config)
stats = exporter.export()
print(f"å¯¼å‡º {stats['exported']} æ¡æ•°æ®åˆ° {stats['export_file']}")
```

### 3. æŸ¥è¯¢å†å²æ•°æ®

```python
import pandas as pd

# è¯»å– Parquet æ–‡ä»¶
df = pd.read_parquet('data_exports/data_20241020_120000.parquet')

# ç­›é€‰
df_rss = df[df['source'] == 'rss']
df_china = df[df['feed_category'] == 'china']
```

---

## âœ… å®æ–½æ­¥éª¤

1. âœ… æ›´æ–° `config.example.yaml` - æ·»åŠ å­˜å‚¨ä¼˜åŒ–é…ç½®
2. âœ… æ›´æ–° `utils/redis_client.py` - æ·»åŠ ç²¾ç®€æ¨¡å¼
3. âœ… åˆ›å»º `utils/smart_exporter.py` - æ™ºèƒ½å¯¼å‡ºå™¨
4. âœ… æ›´æ–° `control_center.py` - é›†æˆè‡ªåŠ¨å¯¼å‡º
5. â­ï¸ æµ‹è¯•è¿è¡Œ
6. â­ï¸ ç›‘æ§å†…å­˜ä½¿ç”¨

---

**ä¸‹ä¸€æ­¥:** å®æ–½ä»£ç ä¿®æ”¹
