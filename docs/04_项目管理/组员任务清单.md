# 📋 数据爬虫项目 - 组员任务清单

**项目名称:** 财经数据爬虫系统  
**当前阶段:** RSS源修复 + 代码审查  
**负责人:** [填写组员姓名]

---

## 🎯 任务概览

### 任务 1: RSS源手动检查与修复 ⏰ 2-3小时
**优先级:** 🔴 高  
**状态:** ⬜ 待开始

**目标:** 验证并修复6个需要手动检查的RSS源

**详细步骤:**
1. 阅读 `docs/02_使用指南/RSS源手动检查指南.md`
2. 逐个检查6个RSS源（详见指南）
3. 更新 `config.yaml` 中的RSS链接
4. 运行 `python validate_rss_urls.py` 验证
5. 提交修复报告

**交付物:**
- [ ] 更新后的 `config.yaml`
- [ ] RSS源修复报告（Markdown格式）
- [ ] 验证脚本的截图

---

### 任务 2: 测试RSS爬虫 ⏰ 1小时
**优先级:** 🟡 中  
**状态:** ⬜ 待开始

**前置条件:** 完成任务1

**目标:** 确保RSS爬虫能正常抓取数据

**步骤:**
```bash
# 1. 测试单个RSS源
python -c "
from crawlers.rss_crawler import RSSCrawler
import yaml

with open('config.yaml') as f:
    config = yaml.safe_load(f)

crawler = RSSCrawler(config)
crawler.crawl()
"

# 2. 检查Redis中的数据
python -c "
from utils.redis_client import RedisClient

client = RedisClient()
print(f'队列长度: {client.get_queue_length()}')
print(f'数据样例: {client.peek_data(3)}')
"

# 3. 运行10分钟，观察数据量
# 在控制台查看日志输出
```

**检查项:**
- [ ] RSS爬虫启动无错误
- [ ] 数据成功推送到Redis
- [ ] 数据格式正确（包含17个字段）
- [ ] 没有重复数据
- [ ] 日志输出正常

**交付物:**
- [ ] 测试报告（包含数据样例）
- [ ] Redis数据截图
- [ ] 发现的问题列表

---

### 任务 3: 数据源配额测试 ⏰ 30分钟
**优先级:** 🟢 低  
**状态:** ⬜ 待开始

**目标:** 验证所有数据源的配额分配是否合理

**背景:**
配置文件中设置了各数据源的Redis存储配额：
- Reddit: 75%
- RSS: 15%
- NewsAPI: 2%
- Twitter: 5%
- StockTwits: 3%

**步骤:**
```python
# 运行所有爬虫24小时后，统计各源数据量
from utils.redis_client import RedisClient
import json

client = RedisClient()
data = client.peek_data(10000)  # 取1万条样本

# 统计各源比例
sources = {}
for item in data:
    item_dict = json.loads(item) if isinstance(item, str) else item
    source = item_dict.get('source', 'unknown')
    sources[source] = sources.get(source, 0) + 1

total = sum(sources.values())
for source, count in sources.items():
    percent = count / total * 100
    print(f"{source}: {count}条 ({percent:.1f}%)")
```

**检查项:**
- [ ] 各源数据量符合预期比例
- [ ] Reddit没有占用过多空间
- [ ] 小众源（NewsAPI等）数据量合理

**如果比例不合理:**
1. 调整 `config.yaml` 中的 `source_quotas`
2. 或调整各爬虫的抓取频率（`scheduler` 部分）

**交付物:**
- [ ] 数据源分布统计表
- [ ] 配额调整建议（如需要）

---

### 任务 4: 添加数据去重功能 ⏰ 1-2小时
**优先级:** 🟡 中  
**状态:** ⬜ 待开始

**目标:** 防止重复数据进入Redis

**问题:** 
当前系统可能会抓取到重复的新闻（相同URL或标题）

**解决方案:**

#### 方式 1: Redis 去重（推荐）
```python
# 在 utils/redis_client.py 中添加
def push_data(self, data: Dict[str, Any]) -> bool:
    """推送数据（带去重）"""
    try:
        # 生成唯一ID
        unique_id = self._generate_unique_id(data)
        
        # 检查是否已存在
        if self.client.sismember('data_ids', unique_id):
            logger.debug(f"数据已存在，跳过: {unique_id}")
            return False
        
        # 添加到ID集合
        self.client.sadd('data_ids', unique_id)
        
        # 设置ID过期时间（7天）
        self.client.expire('data_ids', 604800)
        
        # 推送数据
        json_data = json.dumps(data, ensure_ascii=False)
        self.client.lpush(self.queue_name, json_data)
        return True
    except Exception as e:
        logger.error(f"推送数据失败: {e}")
        return False

def _generate_unique_id(self, data: Dict[str, Any]) -> str:
    """生成数据唯一ID"""
    import hashlib
    
    # 使用URL作为唯一标识（如果有）
    if data.get('url'):
        return hashlib.md5(data['url'].encode()).hexdigest()
    
    # 否则使用标题+来源
    text = f"{data.get('title', '')}_{data.get('source', '')}"
    return hashlib.md5(text.encode()).hexdigest()
```

#### 方式 2: 爬虫层去重
在每个爬虫中添加URL缓存

**步骤:**
1. 在 `utils/redis_client.py` 添加上述代码
2. 测试去重功能
3. 运行爬虫，观察是否还有重复

**测试:**
```python
# 测试去重
client = RedisClient()

# 推送相同数据两次
data = {'url': 'http://example.com', 'title': 'Test'}
result1 = client.push_data(data)  # 应该成功
result2 = client.push_data(data)  # 应该失败（已存在）

print(f"第一次: {result1}, 第二次: {result2}")
# 预期: 第一次: True, 第二次: False
```

**交付物:**
- [ ] 修改后的 `redis_client.py`
- [ ] 去重测试报告
- [ ] 去重率统计（减少了多少重复数据）

---

### 任务 5: 监控与告警 ⏰ 1小时
**优先级:** 🟢 低  
**状态:** ⬜ 待开始

**目标:** 添加系统监控，及时发现问题

**功能清单:**

#### 1. Redis 内存监控
```python
# 在 control_center.py 添加
def monitor_redis(self):
    """监控Redis状态"""
    memory = self.redis_client.get_memory_usage()
    queue_len = self.redis_client.get_queue_length()
    
    # 警告阈值
    if memory['used_memory_mb'] > 900:
        logger.warning(f"⚠️ Redis内存过高: {memory['used_memory_mb']}MB")
    
    if queue_len > 480000:
        logger.warning(f"⚠️ 队列接近上限: {queue_len}")
    
    # 记录到文件
    with open('logs/redis_monitor.log', 'a') as f:
        f.write(f"{datetime.now()},{memory['used_memory_mb']},{queue_len}\n")
```

#### 2. 爬虫健康检查
```python
def health_check(self):
    """检查所有爬虫状态"""
    status = {
        'reddit': self._check_crawler('reddit'),
        'rss': self._check_crawler('rss'),
        'newsapi': self._check_crawler('newsapi'),
        'twitter': self._check_crawler('twitter'),
        'stocktwits': self._check_crawler('stocktwits')
    }
    
    # 如果有爬虫失败，发送告警
    failed = [k for k, v in status.items() if not v]
    if failed:
        logger.error(f"❌ 爬虫失败: {', '.join(failed)}")
```

**步骤:**
1. 在 `control_center.py` 添加监控函数
2. 在主循环中定期调用（如每10分钟）
3. 测试告警功能

**交付物:**
- [ ] 监控代码
- [ ] 告警测试截图
- [ ] 监控日志样例

---

## 📊 进度追踪

| 任务 | 预计时间 | 实际时间 | 状态 | 完成日期 |
|------|---------|---------|------|---------|
| RSS源修复 | 2-3h | | ⬜ | |
| 爬虫测试 | 1h | | ⬜ | |
| 配额测试 | 30min | | ⬜ | |
| 数据去重 | 1-2h | | ⬜ | |
| 监控告警 | 1h | | ⬜ | |

**总计:** 5.5-7.5小时

---

## ✅ 检查清单

完成所有任务后，确认以下事项：

### 代码质量
- [ ] 所有RSS链接已验证可用
- [ ] 没有重复代码
- [ ] 没有无用的注释代码
- [ ] 变量命名规范
- [ ] 函数有文档字符串

### 功能测试
- [ ] RSS爬虫正常工作
- [ ] 数据成功存入Redis
- [ ] 数据格式正确
- [ ] 去重功能正常
- [ ] 监控正常运行

### 文档
- [ ] RSS修复报告已提交
- [ ] 测试报告已提交
- [ ] 代码有足够注释
- [ ] README已更新

### 配置
- [ ] `config.yaml` 已更新
- [ ] 所有API密钥已配置（测试环境）
- [ ] Redis配置正确

---

## 📤 提交内容

完成后，提交以下文件到项目仓库：

```
提交内容:
├── config.yaml (更新后的配置)
├── utils/redis_client.py (如有修改)
├── control_center.py (如有修改)
├── reports/
│   ├── RSS修复报告.md
│   ├── 爬虫测试报告.md
│   ├── 数据源配额分析.md
│   └── 去重功能测试.md
└── screenshots/
    ├── redis_data.png
    ├── crawler_logs.png
    └── monitoring.png
```

**提交信息格式:**
```
fix: 修复RSS源并添加去重功能

- 修复6个RSS源的链接
- 添加Redis数据去重
- 优化数据源配额分配
- 添加系统监控
```

---

## 🆘 遇到问题？

### 技术问题
- 查看 `docs/` 目录下的相关文档
- 检查 `logs/` 目录下的日志文件
- 联系项目负责人

### 时间问题
- 优先完成任务1和任务2
- 任务3-5可以根据时间调整

### 资源问题
- Redis服务未启动：`redis-server`
- Python包缺失：`pip install -r requirements.txt`

---

**开始时间:** [填写]  
**预计完成:** [填写]  
**实际完成:** [填写]

**加油！** 💪
